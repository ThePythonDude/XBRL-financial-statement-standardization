{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b660446b",
   "metadata": {},
   "source": [
    "## Download and extract a 10k/10q XBRL .zip file from the SECs website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10617868",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d769f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package to create directories\n",
    "import os\n",
    "# package used to delete zip files\n",
    "import shutil\n",
    "# package for requests\n",
    "import requests\n",
    "# libary to open URLs\n",
    "import urllib.request\n",
    "# package to work with data frames and store data in .csv files\n",
    "import pandas as pd\n",
    "# package to initiate breaks\n",
    "import time\n",
    "# package to randomize the length of the breaks\n",
    "from random import uniform\n",
    "# package to get the year and the number of the week\n",
    "from datetime import datetime\n",
    "# import package to work with regular expressions\n",
    "import re\n",
    "# package to work with .zip files\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d6dab",
   "metadata": {},
   "source": [
    "### Specify user-agent and directory to save files\n",
    "\n",
    "The layout should be \"Sample Company Name AdminContact@<sample company domain>.com\"\n",
    "https://www.sec.gov/os/webmaster-faq#developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6acd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = ADD_THE_NAME_HERE_AS_STRING\n",
    "contact_mail = ADD_THE_MAIL_HERE_AS_STRING\n",
    "\n",
    "user_agent = \"%s %s\"%(company_name,contact_mail)\n",
    "###\n",
    "directory = ADD_DIRECTORY_HERE_AS_STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5cd3b",
   "metadata": {},
   "source": [
    "### Initiate the class SEC Database to download the files\n",
    "\n",
    "1. When initiating the class, the user-agent to download the files as well as the output directory are stored.\n",
    "2. On a weekly basis, a list with a firm \"cik\", \"name\", \"ticker\" and \"exchange\" is downloaded. Old files are deleted.\n",
    "3. On a weekly basis, a list with all fillings, including their \"accessionNumber\", \"filingDate\", \"reportDate\" and XBRL information is downloaded. Old files are deleted.\n",
    "4. Download all XBRL fillings of a firm as a .zip and extract the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97534e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEC_DB: \n",
    "    ############################################################################################################################\n",
    "    # 1.\n",
    "    # When the class is initiated, create the directory to save the files\n",
    "    def init(self, directory, header):\n",
    "        # create the directory if it doesn't exist        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # define the Data Base variable (DB)\n",
    "        self.DB = directory\n",
    "        \n",
    "        # define header\n",
    "        self.hdr = header\n",
    "        \n",
    "    ############################################################################################################################    \n",
    "    # 2.\n",
    "    # create function which returns the number of the week in the current year\n",
    "    def todays_week(self):  \n",
    "        calender = datetime.today().isocalendar()\n",
    "        return '%s-%s'%(calender[0], calender[1])\n",
    "    \n",
    "    # check if a download is necessary\n",
    "    def csv_file_exist(self, directory, file_name):\n",
    "        # check if file exists:\n",
    "        boolean = os.path.isfile('%s\\%s_%s.csv'%(directory, file_name, self.todays_week()))\n",
    "        return boolean\n",
    "    \n",
    "    # check if there are older files to delete\n",
    "    def older_csv_file_exist(self, directory, file_name):\n",
    "        # check if older file exists, with the same prefix but another suffix\n",
    "        for fname in os.listdir('%s'%(directory)):\n",
    "            if ((fname.startswith('%s'%(file_name)) == True) and\n",
    "                (fname.endswith('%s.csv'%(self.todays_week())) == False)):\n",
    "                # if older file exists, delete it\n",
    "                os.remove('%s\\%s'%(directory, fname))    \n",
    "    \n",
    "    # define function, to download master company key file, \n",
    "    # if the newest is not already on disk     \n",
    "    def edgar_companies(self, file_name='EdgarCompanies'):\n",
    "        save_path = self.DB\n",
    "        # check if it already exists\n",
    "        if self.csv_file_exist(self.DB, file_name) != True:\n",
    "            \n",
    "            time.sleep(uniform(1, 2)) # break between 1 - 2 seconds            \n",
    "\n",
    "            # link to the overview of companies (no funds) in the edgar database\n",
    "            URL = 'https://www.sec.gov/files/company_tickers_exchange.json'\n",
    "            # save data to JSON\n",
    "            r = requests.get(URL, headers=self.hdr)\n",
    "            data = r.json()\n",
    "\n",
    "            # extract the features\n",
    "            # headers are stored in the \"fields\" section and values in the \"item\" section\n",
    "            df = pd.DataFrame([{data['fields'][0]: item[0], data['fields'][1]: item[1],\n",
    "                                data['fields'][2]: item[2], data['fields'][3]: item[3],} \n",
    "                               for item in data['data'] ])\n",
    "\n",
    "            # save the file as a .csv\n",
    "            df.to_csv('%s/%s_%s.csv'%(save_path, file_name, self.todays_week()), index=False)\n",
    "            \n",
    "            # check if there is already an existing file to delete\n",
    "            self.older_csv_file_exist(self.DB, file_name)            \n",
    "\n",
    "            return print(df,\"csv file saved at %s with the name %s\"%(save_path, file_name))\n",
    "        \n",
    "        else:\n",
    "            return print(\"Key file was already downloaded this week\")\n",
    "        \n",
    "    ############################################################################################################################\n",
    "    # 3. \n",
    "    # define function, to return all possible submissions of a firm\n",
    "    def return_submissions(self, ticker, file_name='submissions'):\n",
    "        # create a subfolder for the ticker\n",
    "        if not os.path.exists('%s/%s'%(self.DB, ticker)):\n",
    "            os.makedirs('%s/%s'%(self.DB, ticker))\n",
    "            \n",
    "        # check if it already exists\n",
    "        if self.csv_file_exist('%s/%s'%(self.DB, ticker), file_name) != True:            \n",
    "            # get the CIK for the ticker   \n",
    "            key_file = pd.read_csv('%s/%s'%(self.DB, 'EdgarCompanies_%s.csv'%(self.todays_week())))      \n",
    "            cik = str(key_file.loc[key_file['ticker']==ticker, 'cik'].values[0])\n",
    "\n",
    "            # the cik is always used with 10 digits. Therefore zeros are missing\n",
    "            missing_zero = (10 - len(cik)) * '0'        \n",
    "            complete_cik = missing_zero + cik\n",
    "\n",
    "            # open recent submission .json file        \n",
    "            URL = 'https://data.sec.gov/submissions/CIK%s.json'%(complete_cik)\n",
    "            # request and save data to JSON\n",
    "            r = requests.get(URL, headers=self.hdr)\n",
    "            data = r.json()        \n",
    "            submissions = pd.DataFrame(data['filings']['recent'])\n",
    "\n",
    "            for add_sub in data['filings']['files']:\n",
    "                add_json = add_sub['name']\n",
    "\n",
    "                time.sleep(uniform(1, 2)) # break between 1 - 2 seconds     \n",
    "\n",
    "                # open recent submission .json file        \n",
    "                URL = 'https://data.sec.gov/submissions/%s'%(add_json)\n",
    "                # request and save data to JSON\n",
    "                r = requests.get(URL, headers=self.hdr)\n",
    "                add_data = r.json()                  \n",
    "                # add the additional submissons\n",
    "                submissions = pd.concat([submissions, pd.DataFrame(add_data)], ignore_index=True)\n",
    "\n",
    "            # save the submissions\n",
    "            submissions.sort_values('filingDate', inplace=True, ascending=False)        \n",
    "            submissions.to_csv(\"%s/%s\\submissions_%s.csv\"%(self.DB, ticker, self.todays_week()))\n",
    "\n",
    "            # check if there is already an existing file to delete\n",
    "            self.older_csv_file_exist('%s/%s'%(self.DB, ticker), file_name) \n",
    "        else:\n",
    "            return print(\"Submission file was already downloaded this week\")\n",
    "        \n",
    "    ############################################################################################################################\n",
    "    # 4. \n",
    "    # Download all XBRL fillings of a firm as a .zip and extract the files\n",
    "    def download_zip_submission(self, ticker, form, start_year=2010, subm_df_name='submissions'):\n",
    "        # get the cik for the ticker\n",
    "        key_file = pd.read_csv('%s/%s'%(self.DB, 'EdgarCompanies_%s.csv'%(self.todays_week())))      \n",
    "        cik = str(key_file.loc[key_file['ticker']==ticker, 'cik'].values[0])\n",
    "        \n",
    "        # open the saved submission df\n",
    "        for fname in os.listdir('%s/%s'%(self.DB, ticker)):\n",
    "            if fname.startswith('%s'%(subm_df_name)) == True:\n",
    "                name_sub_df = fname\n",
    "        submissions = pd.read_csv('%s/%s/%s'%(self.DB, ticker, name_sub_df))   \n",
    "        submissions['reportDate'] =  pd.to_datetime(submissions['reportDate'], format='%Y-%m-%d')\n",
    "        # get all the relevant accession numbers, which are also available as an XBRL document\n",
    "        submissions = submissions.loc[((submissions['reportDate'].dt.year >= start_year) &\n",
    "                                       (submissions['form'] == form)&(submissions['isXBRL'] == 1))]        \n",
    "        acc_numbers = submissions['accessionNumber'].to_list()      \n",
    "        \n",
    "        # folder names\n",
    "        # list of all content in a directory, filtered so only directories are returned\n",
    "        # from: https://stackoverflow.com/questions/29206384/python-folder-names-in-the-directory\n",
    "        path = '%s/%s/'%(self.DB, ticker)\n",
    "        dir_list = [directory for directory in os.listdir(path) if os.path.isdir(path+directory)]\n",
    "        \n",
    "        # download only the files, which weren't already downloaded\n",
    "        acc_numbers = [a for a in acc_numbers if a not in dir_list]\n",
    "        \n",
    "        # if the list is not empty\n",
    "        if acc_numbers:        \n",
    "            for number in acc_numbers:\n",
    "                # remove the dash of the number\n",
    "                no_dash_num = re.sub('-', '', number)\n",
    "\n",
    "                # URL of the .ZIP of the filing\n",
    "                url = 'https://www.sec.gov/Archives/edgar/data/%s/%s/%s-xbrl.zip'%(cik, no_dash_num, number)\n",
    "\n",
    "                # destination\n",
    "                # create the directory if it doesn't exist  \n",
    "                dst = '%s/%s/ZipDummy'%(self.DB, ticker)\n",
    "                if not os.path.exists(dst):\n",
    "                    os.makedirs(dst)\n",
    "\n",
    "                extraction_folder = '%s/%s/%s'%(self.DB, ticker, number)\n",
    "                # folder to which to extract the files of zip file\n",
    "                if not os.path.exists(extraction_folder):\n",
    "                    os.makedirs(extraction_folder)\n",
    "\n",
    "                time.sleep(uniform(1, 2)) # break between 1 - 2 seconds  \n",
    "                # specify downloader\n",
    "                opener = urllib.request.build_opener()\n",
    "                opener.addheaders = self.hdr.items()\n",
    "                urllib.request.install_opener(opener)\n",
    "                urllib.request.urlretrieve(url, '%s\\%s.zip'%(dst, number))\n",
    "\n",
    "                # extract the zip file\n",
    "                with zipfile.ZipFile('%s/%s.zip'%(dst, number), 'r') as zip_ref:\n",
    "                    zip_ref.extractall('%s'%(extraction_folder))\n",
    "\n",
    "                # delete the zip file after extraction\n",
    "                shutil.rmtree(dst)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3efab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_ticker = 'MSFT'\n",
    "filling_form = '10-K'\n",
    "################################################################################################################################\n",
    "# 1\n",
    "# initiate the class\n",
    "SEC = SEC_DB()\n",
    "\n",
    "# insert directory and user agent\n",
    "SEC.init(directory,\n",
    "         header={'User-Agent': user_agent})\n",
    "# 2\n",
    "SEC.edgar_companies()\n",
    "# 3\n",
    "SEC.return_submissions(ticker=firm_ticker)\n",
    "# 4\n",
    "SEC.download_zip_submission(ticker=firm_ticker, form=filling_form, start_year=2010, subm_df_name='submissions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
